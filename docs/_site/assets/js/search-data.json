{"0": {
    "doc": "Advanced Nodes",
    "title": "Advanced Nodes: Parallel/Batch",
    "content": " ",
    "url": "/Advanced/Advanced_Nodes.html#advanced-nodes-parallelbatch",
    
    "relUrl": "/Advanced/Advanced_Nodes.html#advanced-nodes-parallelbatch"
  },"1": {
    "doc": "Advanced Nodes",
    "title": "Beyond the Basics: Supercharging Your Nodes",
    "content": "Sometimes a simple node just doesn’t cut it. Whether you need to fetch data from multiple APIs simultaneously or process a batch of items, the Node class has got your back. Let’s explore how to take your nodes from “meh” to “magnificent”! . ",
    "url": "/Advanced/Advanced_Nodes.html#beyond-the-basics-supercharging-your-nodes",
    
    "relUrl": "/Advanced/Advanced_Nodes.html#beyond-the-basics-supercharging-your-nodes"
  },"2": {
    "doc": "Advanced Nodes",
    "title": "Embracing Asynchrony: The Async Magic",
    "content": "Any of the core node methods - prepare, execute, or cleanup - can be async. Just slap on that async keyword and you’re good to go: . class AsyncFetchNode(Node): async def execute(self, prepared_result): # Look at me, I'm async! await asyncio.sleep(1) # Simulating network delay return {\"data\": \"Fetched asynchronously\"} . The workflow engine knows how to handle these async methods and will await them properly. No need to worry about the plumbing! . ",
    "url": "/Advanced/Advanced_Nodes.html#embracing-asynchrony-the-async-magic",
    
    "relUrl": "/Advanced/Advanced_Nodes.html#embracing-asynchrony-the-async-magic"
  },"3": {
    "doc": "Advanced Nodes",
    "title": "Going Parallel: Multi-tasking Like a Pro",
    "content": "Want to run multiple operations at the same time? Extend the _execute_with_retry method: . class ParallelNode(Node): async def prepare(self, shared, request_input): # Return a list of items to process in parallel return [{\"id\": i} for i in range(5)] async def _execute_with_retry(self, items): # Process all items in parallel tasks = [self._process_item(item) for item in items] results = await asyncio.gather(*tasks, return_exceptions=True) # Check for exceptions for result in results: if isinstance(result, Exception): raise result return results async def execute(self, item): # Process a single item await asyncio.sleep(1) # Simulate work return f\"Processed item {item['id']}\" def cleanup(self, shared, prepared_result, execution_result): # Initialize results list if it doesn't exist if \"parallel_results\" not in shared: shared[\"parallel_results\"] = [] # Append new results to existing list shared[\"parallel_results\"].extend(execution_result) return execution_result . This node will process all items simultaneously, making your workflow zip along at warp speed! . ",
    "url": "/Advanced/Advanced_Nodes.html#going-parallel-multi-tasking-like-a-pro",
    
    "relUrl": "/Advanced/Advanced_Nodes.html#going-parallel-multi-tasking-like-a-pro"
  },"4": {
    "doc": "Advanced Nodes",
    "title": "Batch Processing: Same Task, Different Data",
    "content": "Don’t need the complexity of parallelism but want to process multiple items? Batch processing is your friend: . class BatchNode(Node): def prepare(self, shared, request_input): # Return batch of items to process return [f\"item-{i}\" for i in range(10)] async def _execute_with_retry(self, items): # Process each item in batch sequentially results = [] for item in items: # Process each item with potential retries result = await super()._process_item(item) results.append(result) return results def execute(self, item): # Process a single item return f\"Processed {item}\" def cleanup(self, shared, prepared_result, execution_result): # Initialize results dictionary if needed if \"batch_results\" not in shared: shared[\"batch_results\"] = [] # Add latest batch results shared[\"batch_results\"].extend(execution_result) return execution_result . ",
    "url": "/Advanced/Advanced_Nodes.html#batch-processing-same-task-different-data",
    
    "relUrl": "/Advanced/Advanced_Nodes.html#batch-processing-same-task-different-data"
  },"5": {
    "doc": "Advanced Nodes",
    "title": "Error Handling in Advanced Nodes",
    "content": "Currently, if any task in a parallel or batch node fails, the entire node fails. This “fail fast” approach keeps things simple and predictable. class ParallelWithErrorNode(Node): async def prepare(self, shared, request_input): return [1, 2, 0, 4] # That zero will cause trouble! async def execute(self, item): # This will fail for item == 0 result = 10 / item return result def exec_fallback(self, prepared_result, exception): # This will run if execute fails return f\"Failed to process items: {exception}\" . If you need more sophisticated error handling (like continuing despite errors in some items), feel free to raise an issue on GitHub. We’re always looking to improve! . Remember: these advanced nodes follow the same lifecycle as basic nodes, just with superpowers. Mix and match these techniques to create workflows that handle real-world complexity with grace! . ",
    "url": "/Advanced/Advanced_Nodes.html#error-handling-in-advanced-nodes",
    
    "relUrl": "/Advanced/Advanced_Nodes.html#error-handling-in-advanced-nodes"
  },"6": {
    "doc": "Advanced Nodes",
    "title": "Advanced Nodes",
    "content": " ",
    "url": "/Advanced/Advanced_Nodes.html",
    
    "relUrl": "/Advanced/Advanced_Nodes.html"
  },"7": {
    "doc": "Debug Mode",
    "title": "Debug Mode",
    "content": " ",
    "url": "/Core/UI/Debug.html",
    
    "relUrl": "/Core/UI/Debug.html"
  },"8": {
    "doc": "Debug Mode",
    "title": "Overview",
    "content": "Debug Mode is Grapheteria’s most powerful feature, making workflow development dramatically easier. When mistakes happen (and they will!), Debug Mode helps you identify and fix issues with minimal frustration through its live state visualization and time-travel capabilities. ",
    "url": "/Core/UI/Debug.html#overview",
    
    "relUrl": "/Core/UI/Debug.html#overview"
  },"9": {
    "doc": "Debug Mode",
    "title": "Visual Elements",
    "content": "State Visualization . Debug Mode reveals the shared state that nodes read from and write to, giving you visibility into your workflow’s data at each step of execution. Node Status Indicators . The canvas displays each node’s current status: . | Queued: Next in line for execution | Pending: Currently being processed | Completed: Successfully executed | Failed: Encountered an error | . ",
    "url": "/Core/UI/Debug.html#visual-elements",
    
    "relUrl": "/Core/UI/Debug.html#visual-elements"
  },"10": {
    "doc": "Debug Mode",
    "title": "Time Travel Controls",
    "content": "Debug Mode’s control panel gives you precise control over workflow execution: . | Step: Execute a single step in the workflow | Run: Progress to the next halt stage (completion or waiting node) | Prev: Travel back to a previous state | Next: Move forward to a future state (if available) | . Time Travel Superpowers . Found a bug? No need to restart! Simply: . | Go back in time with Prev | Fix the problematic code | Resume with Step or Run | . Your workflow picks up exactly where you left off. When stepping forward, future states are overwritten with the new execution path, keeping your debug session consistent. ",
    "url": "/Core/UI/Debug.html#time-travel-controls",
    
    "relUrl": "/Core/UI/Debug.html#time-travel-controls"
  },"11": {
    "doc": "Debug Mode",
    "title": "Node Operations During Debugging",
    "content": "Deletion Rules . To maintain logical consistency during time travel: . | Nodes that have completed, failed, or are waiting for input cannot be removed | To delete a problematic node, travel back to before it executed, then remove it | . Addition Freedom . Feel free to add new nodes or edges at any point in the debugging process—even after a workflow has completed. Then step forward to see how they affect execution! . ",
    "url": "/Core/UI/Debug.html#node-operations-during-debugging",
    
    "relUrl": "/Core/UI/Debug.html#node-operations-during-debugging"
  },"12": {
    "doc": "Debug Mode",
    "title": "Providing Input During Debugging",
    "content": "Input Requests . Some nodes in your workflow might need human input to continue processing. When execution reaches one of these nodes, you’ll see: . | A waiting_for_input badge on the node | Input request details displayed in the node’s info panel | . No amount of clicking Step or Run will advance the workflow until the required input is provided—the machine patiently waits for your wisdom! . Supplying Input Data . To satisfy a waiting node’s hunger for data: . | Click the Configure Input Data button at the bottom of the debug panel | This opens a modal where you can provide the requested information | For your convenience, the modal pre-populates the request ID that’s currently waiting | Enter your input value (which can be text, numbers, or even structured data depending on the node) | Submit the input to let the workflow continue its journey | . After providing input, the node transitions from its waiting state and the workflow resumes execution. You can then continue stepping through your workflow as normal. ",
    "url": "/Core/UI/Debug.html#providing-input-during-debugging",
    
    "relUrl": "/Core/UI/Debug.html#providing-input-during-debugging"
  },"13": {
    "doc": "Debug Mode",
    "title": "Error Handling",
    "content": "Debug Mode displays error messages and exceptions directly in the UI, making it easy to identify what went wrong and where. Common errors include invalid inputs, missing connections, or logic problems in your code. With time travel, you can step back, fix the issue, and continue without restarting. ",
    "url": "/Core/UI/Debug.html#error-handling",
    
    "relUrl": "/Core/UI/Debug.html#error-handling"
  },"14": {
    "doc": "Debug Mode",
    "title": "Known Issues",
    "content": "Sometimes you might encounter persistent errors (like a corrupted state save) where even error messages get stuck in a loop. In these cases, it’s best to start a fresh debug session after fixing the underlying issue. For solutions to common problems, check the Troubleshooting Guide. ",
    "url": "/Core/UI/Debug.html#known-issues",
    
    "relUrl": "/Core/UI/Debug.html#known-issues"
  },"15": {
    "doc": "Deploying Workflows",
    "title": "Deploying Workflows with FastAPI",
    "content": " ",
    "url": "/Advanced/Deployment.html#deploying-workflows-with-fastapi",
    
    "relUrl": "/Advanced/Deployment.html#deploying-workflows-with-fastapi"
  },"16": {
    "doc": "Deploying Workflows",
    "title": "Overview",
    "content": "Ready to share your workflows with the world? Here we show examples with FastAPI (although any library works) to make it easy to deploy your Grapheteria workflows as a flexible API. Whether you’re building an internal tool or a public service, these routes will help you manage workflow creation, execution, and monitoring. First, let’s create a simple workflow definition file that we can use in our API: . # workflow_definitions.py from grapheteria import Node, WorkflowEngine class StartNode(Node): def execute(self, prepared_result): return \"Hello from the start node!\" class ProcessNode(Node): def execute(self, prepared_result): return f\"Processing data: {self.config.get('process_type', 'default')}\" class EndNode(Node): def execute(self, prepared_result): return \"Workflow completed successfully!\" def create_sample_workflow(): # Create nodes start = StartNode(id=\"start_node_1\") process = ProcessNode(id=\"process_node_1\", config={\"process_type\": \"sample\"}) end = EndNode(id=\"end_node_1\") # Connect nodes start &gt; process &gt; end # Create workflow with nodes and start node return WorkflowEngine(nodes=[start, process, end], start=start) . Here’s the JSON equivalent of this workflow that could be stored as a file: . { \"nodes\": [ { \"id\": \"start_node_1\", \"class\": \"StartNode\" }, { \"id\": \"process_node_1\", \"class\": \"ProcessNode\", \"config\": { \"process_type\": \"sample\" } }, { \"id\": \"end_node_1\", \"class\": \"EndNode\" } ], \"edges\": [ { \"from\": \"start_node_1\", \"to\": \"process_node_1\" }, { \"from\": \"process_node_1\", \"to\": \"end_node_1\" } ], \"start\": \"start_node_1\" } . This JSON could be created and exported using the Grapheteria UI . Now, let’s set up our FastAPI app: . from fastapi import FastAPI, HTTPException, Body from grapheteria import WorkflowEngine, WorkflowStatus from typing import Dict, Any, Optional import asyncio #Import required for both code-based or JSON-based from workflow_definitions import create_sample_workflow # Create FastAPI app app = FastAPI(title=\"Grapheteria Workflows API\") # Dictionary to store active workflows active_workflows = {} . ",
    "url": "/Advanced/Deployment.html#overview",
    
    "relUrl": "/Advanced/Deployment.html#overview"
  },"17": {
    "doc": "Deploying Workflows",
    "title": "Creating Workflows",
    "content": "Create a new workflow instance with this route: . @app.get(\"/workflows/create/{workflow_id}\") async def create_workflow(workflow_id: str): try: # Create workflow from JSON definition workflow = WorkflowEngine(workflow_id=workflow_id) # Note: You could also create a workflow from code: # workflow = create_sample_workflow() # Get the run ID and store workflow in memory run_id = workflow.run_id active_workflows[(workflow_id, run_id)] = workflow return { \"message\": \"Workflow created\", \"run_id\": run_id, \"execution_data\": workflow.tracking_data } except Exception as e: raise HTTPException(status_code=500, detail=f\"Failed to create workflow: {str(e)}\") . This endpoint creates a workflow instance and returns a run_id you’ll need for subsequent operations. ",
    "url": "/Advanced/Deployment.html#creating-workflows",
    
    "relUrl": "/Advanced/Deployment.html#creating-workflows"
  },"18": {
    "doc": "Deploying Workflows",
    "title": "Stepping Through a Workflow",
    "content": "Sometimes you want more control over execution. This route lets you step through one node at a time: . @app.post(\"/workflows/step/{workflow_id}/{run_id}\") async def step_workflow( workflow_id: str, run_id: str, input_data: Optional[Dict[str, Any]] = Body(None), resume_from: Optional[int] = Body(None), fork: bool = Body(False) ): # Either get existing workflow or create new one if resume_from is None and not fork and (workflow_id, run_id) in active_workflows: workflow = active_workflows[(workflow_id, run_id)] else: workflow = WorkflowEngine( workflow_id=workflow_id, run_id=run_id, resume_from=resume_from, fork=fork ) active_workflows[(workflow_id, run_id)] = workflow # Execute a single step try: continuing, tracking_data = await workflow.step(input_data=input_data) return { \"message\": \"Workflow stepped\", \"continuing\": continuing, \"execution_data\": tracking_data } except Exception as e: raise HTTPException(status_code=500, detail=f\"Step failed: {str(e)}\") . Perfect for cautious workflows or when you need to process each node’s results separately. ",
    "url": "/Advanced/Deployment.html#stepping-through-a-workflow",
    
    "relUrl": "/Advanced/Deployment.html#stepping-through-a-workflow"
  },"19": {
    "doc": "Deploying Workflows",
    "title": "Running a Complete Workflow",
    "content": "When you’re ready to let your workflow run until completion or until it needs input: . @app.post(\"/workflows/run/{workflow_id}/{run_id}\") async def run_workflow( workflow_id: str, run_id: str, input_data: Optional[Dict[str, Any]] = Body(None), resume_from: Optional[int] = Body(None), fork: bool = Body(False) ): # Get existing or create new workflow if resume_from is None and not fork and (workflow_id, run_id) in active_workflows: workflow = active_workflows[(workflow_id, run_id)] else: workflow = WorkflowEngine( workflow_id=workflow_id, run_id=run_id, resume_from=resume_from, fork=fork ) active_workflows[(workflow_id, run_id)] = workflow # Run until completion or waiting for input try: continuing, tracking_data = await workflow.run(input_data=input_data) return { \"message\": \"Workflow run\", \"continuing\": continuing, \"execution_data\": tracking_data, \"status\": workflow.execution_state.workflow_status.name } except Exception as e: raise HTTPException(status_code=500, detail=f\"Execution failed: {str(e)}\") . Set it and forget it! Your workflow runs until it needs input or reaches its destination. ",
    "url": "/Advanced/Deployment.html#running-a-complete-workflow",
    
    "relUrl": "/Advanced/Deployment.html#running-a-complete-workflow"
  },"20": {
    "doc": "Deploying Workflows",
    "title": "Additional Useful Routes",
    "content": "Getting Workflow Status . @app.get(\"/workflows/status/{workflow_id}/{run_id}\") async def get_workflow_status(workflow_id: str, run_id: str): \"\"\"Check the current status of a workflow\"\"\" try: # Load the workflow directly with WorkflowEngine workflow = WorkflowEngine(workflow_id=workflow_id, run_id=run_id) return { \"status\": workflow.execution_state.workflow_status.name, \"awaiting_input\": workflow.execution_state.awaiting_input, \"current_node\": workflow.execution_state.next_node_id } except Exception as e: raise HTTPException(status_code=404, detail=f\"Workflow not found: {str(e)}\") # Note: Alternatively, we could load directly from storage logs # storage = FileSystemStorage() # state = storage.load_state(workflow_id, run_id) . Perfect for checking if your workflow is lounging on the beach or hard at work. Viewing Workflow Logs . Get insights into your workflows with these logging endpoints: . @app.get(\"/logs\") async def get_logs(): \"\"\"List all available workflows\"\"\" from grapheteria.utils import FileSystemStorage storage = FileSystemStorage() return storage.list_workflows() @app.get(\"/logs/{workflow_id}\") async def get_workflow_logs(workflow_id: str): \"\"\"List all runs for a specific workflow\"\"\" from grapheteria.utils import FileSystemStorage storage = FileSystemStorage() return storage.list_runs(workflow_id) @app.get(\"/logs/{workflow_id}/{run_id}\") async def get_run_logs(workflow_id: str, run_id: str): \"\"\"Get full execution history for a specific run\"\"\" from grapheteria.utils import FileSystemStorage storage = FileSystemStorage() return storage.load_state(workflow_id, run_id) . These routes let you peek under the hood to see what your workflows have been up to. ",
    "url": "/Advanced/Deployment.html#additional-useful-routes",
    
    "relUrl": "/Advanced/Deployment.html#additional-useful-routes"
  },"21": {
    "doc": "Deploying Workflows",
    "title": "Running Your API",
    "content": "With these routes in place, you can launch your API server: . if __name__ == \"__main__\": import uvicorn uvicorn.run(app, host=\"0.0.0.0\", port=8000) . Now your workflows are ready for the big time! Access them via HTTP requests from any application or try them out using FastAPI’s automatic Swagger UI at http://localhost:8000/docs. For more details on deploying your FastAPI application to production, check out the FastAPI deployment documentation . ",
    "url": "/Advanced/Deployment.html#running-your-api",
    
    "relUrl": "/Advanced/Deployment.html#running-your-api"
  },"22": {
    "doc": "Deploying Workflows",
    "title": "Deploying Workflows",
    "content": " ",
    "url": "/Advanced/Deployment.html",
    
    "relUrl": "/Advanced/Deployment.html"
  },"23": {
    "doc": "Edge",
    "title": "Connect your Nodes",
    "content": " ",
    "url": "/Core/Edge.html#connect-your-nodes",
    
    "relUrl": "/Core/Edge.html#connect-your-nodes"
  },"24": {
    "doc": "Edge",
    "title": "Overview",
    "content": "Edges are the connections between nodes in your workflow graph. They determine how your workflow transitions from one node to another. Without edges, your nodes would be isolated islands of functionality with no way to reach each other. Each edge has access to the workflow’s shared communication state, allowing for dynamic routing decisions based on your data. # Creating an edge between two nodes start_node &gt; process_node &gt; end_node . ",
    "url": "/Core/Edge.html#overview",
    
    "relUrl": "/Core/Edge.html#overview"
  },"25": {
    "doc": "Edge",
    "title": "Conditions",
    "content": "Each edge in your workflow is a one-way street connecting two nodes with some conditions that determine when traffic can flow. These conditions are Python expressions (as strings) that evaluate to True or False based on the current workflow’s shared state. # Edge with a condition validate_node - \"shared['score'] &gt; 80\" &gt; success_node validate_node - \"shared['score'] &lt;= 80\" &gt; retry_node . The default edge (with an empty string condition \"\") serves as a fallback path when no other conditions match. Conditions make your workflow dynamic, enabling complex branching logic. ",
    "url": "/Core/Edge.html#conditions",
    
    "relUrl": "/Core/Edge.html#conditions"
  },"26": {
    "doc": "Edge",
    "title": "Order of Edge Condition Evaluation",
    "content": "When a node finishes execution, the system evaluates its outgoing edges in this order: . | True condition: If any edge has the literal condition \"True\", it’s automatically selected regardless of other edges. # This edge will always be taken special_node - \"True\" &gt; priority_node . | Evaluated conditions: The system evaluates each edge’s condition against the shared state and selects the first one that returns True. # First matching condition wins decision_node - \"shared['temp'] &gt; 30\" &gt; hot_handler decision_node - \"shared['temp'] &gt; 20\" &gt; warm_handler . | Default edge: If no conditions match, the system uses the default edge (empty condition) as a fallback. decision_node &gt; default_handler # Default edge (empty condition) . | . # Complex condition example analysis_node - \"shared['status'] == 'urgent' and shared['priority'] &gt; 5\" &gt; urgent_handler analysis_node &gt; standard_handler . Remember, edge conditions are the decision points in your workflow - they determine which path your data will travel! . ",
    "url": "/Core/Edge.html#order-of-edge-condition-evaluation",
    
    "relUrl": "/Core/Edge.html#order-of-edge-condition-evaluation"
  },"27": {
    "doc": "Edge",
    "title": "JSON Definition",
    "content": "While Python code is great for programmatically building workflows, you can also define edges in JSON. This is especially handy when working with the UI editor (Grapheteria’s center of attraction!), which syncs with and can modify your JSON schema in real-time. { \"edges\": [ { \"from\": \"validate_node\", \"to\": \"success_node\", \"condition\": \"shared['score'] &gt; 80\" }, { \"from\": \"validate_node\", \"to\": \"retry_node\", \"condition\": \"shared['score'] &lt;= 80\" }, { \"from\": \"process_node\", \"to\": \"end_node\" } ] } . Note that in JSON, the “from” and “to” fields are string IDs that reference nodes by their identifier, not the actual node objects as in code. This is a key difference between the two approaches. The last edge has no condition specified - it’s our default edge! The JSON representation makes it easy to visualize your entire workflow structure in one place. Now you’re ready to connect your nodes any way you like - with code or JSON! Whether you’re building a simple linear process or a complex decision tree, edges are your trusty pathways through the workflow jungle. ",
    "url": "/Core/Edge.html#json-definition",
    
    "relUrl": "/Core/Edge.html#json-definition"
  },"28": {
    "doc": "Edge",
    "title": "Edge",
    "content": " ",
    "url": "/Core/Edge.html",
    
    "relUrl": "/Core/Edge.html"
  },"29": {
    "doc": "Event-Driven Workflows",
    "title": "Making Your Workflow Event-Driven",
    "content": " ",
    "url": "/Advanced/Event_Driven.html#making-your-workflow-event-driven",
    
    "relUrl": "/Advanced/Event_Driven.html#making-your-workflow-event-driven"
  },"30": {
    "doc": "Event-Driven Workflows",
    "title": "Overview",
    "content": "Sometimes you want your workflows to kick into action when something happens in the outside world. Maybe an order comes in, a temperature sensor hits a threshold, or your cat posts a new Instagram photo. Here’s how to make your workflows respond to events, not just commands. ",
    "url": "/Advanced/Event_Driven.html#overview",
    
    "relUrl": "/Advanced/Event_Driven.html#overview"
  },"31": {
    "doc": "Event-Driven Workflows",
    "title": "Creating an Event Dispatcher",
    "content": "First, let’s build a robust event dispatcher that can trigger workflows when events occur and persist trigger configurations: . # event_dispatcher.py import asyncio import json import os from datetime import datetime from typing import Dict, List, Any, Optional from grapheteria import WorkflowEngine class EventDispatcher: \"\"\"Manages event subscriptions and dispatches events to workflows.\"\"\" def __init__(self, storage_path=\"events\"): self.storage_path = storage_path self.subscriptions = {} # event_type -&gt; list of workflow IDs self.active_workflows = {} # (workflow_id, run_id) -&gt; WorkflowEngine # Create storage directory if it doesn't exist os.makedirs(storage_path, exist_ok=True) # Load existing subscriptions self.load_triggers() def subscribe(self, event_type: str, workflow_id: str, config: Optional[Dict[str, Any]] = None) -&gt; Dict[str, Any]: \"\"\"Subscribe a workflow to an event type with optional configuration.\"\"\" if event_type not in self.subscriptions: self.subscriptions[event_type] = [] # Create subscription record subscription = { \"workflow_id\": workflow_id, \"config\": config or {}, \"created_at\": datetime.now().isoformat() } self.subscriptions[event_type].append(subscription) # Persist to storage self.save_triggers() return subscription def unsubscribe(self, event_type: str, workflow_id: str) -&gt; bool: \"\"\"Unsubscribe a workflow from an event type.\"\"\" if event_type not in self.subscriptions: return False original_length = len(self.subscriptions[event_type]) self.subscriptions[event_type] = [ sub for sub in self.subscriptions[event_type] if sub[\"workflow_id\"] != workflow_id ] # If the list is empty, remove the event type if not self.subscriptions[event_type]: del self.subscriptions[event_type] # Persist changes self.save_triggers() # Return true if something was removed return len(self.subscriptions.get(event_type, [])) &lt; original_length def get_subscriptions(self, event_type: str = None) -&gt; Dict[str, List[Dict[str, Any]]]: \"\"\"Get all subscriptions or subscriptions for a specific event type.\"\"\" if event_type: return {event_type: self.subscriptions.get(event_type, [])} return self.subscriptions def save_triggers(self) -&gt; None: \"\"\"Save trigger configurations to filesystem.\"\"\" triggers_file = os.path.join(self.storage_path, \"triggers.json\") # Use atomic write operation temp_file = f\"{triggers_file}.tmp\" with open(temp_file, 'w') as f: json.dump(self.subscriptions, f, indent=2) # Atomic rename os.rename(temp_file, triggers_file) def load_triggers(self) -&gt; None: \"\"\"Load trigger configurations from filesystem.\"\"\" triggers_file = os.path.join(self.storage_path, \"triggers.json\") if os.path.exists(triggers_file): try: with open(triggers_file, 'r') as f: self.subscriptions = json.load(f) except json.JSONDecodeError: # Handle corrupted file self.subscriptions = {} else: self.subscriptions = {} async def dispatch(self, event_type: str, payload: Any = None) -&gt; List[Dict[str, Any]]: \"\"\"Dispatch an event to all subscribed workflows.\"\"\" results = [] if event_type not in self.subscriptions: return results for subscription in self.subscriptions[event_type]: workflow_id = subscription[\"workflow_id\"] config = subscription[\"config\"] # Create workflow instance workflow = WorkflowEngine(workflow_id=workflow_id) run_id = workflow.run_id # Store in active workflows self.active_workflows[(workflow_id, run_id)] = workflow # Add event data to shared state workflow.execution_state.shared.update({ \"event_type\": event_type, \"event_payload\": payload, \"event_config\": config, \"event_time\": datetime.now().isoformat() }) # Run the workflow continuing, tracking_data = await workflow.run() results.append({ \"workflow_id\": workflow_id, \"run_id\": run_id, \"status\": workflow.execution_state.workflow_status.name }) return results . ",
    "url": "/Advanced/Event_Driven.html#creating-an-event-dispatcher",
    
    "relUrl": "/Advanced/Event_Driven.html#creating-an-event-dispatcher"
  },"32": {
    "doc": "Event-Driven Workflows",
    "title": "Adding Event Routes to FastAPI",
    "content": "Now, let’s extend your FastAPI application to handle event subscriptions and triggers: . # routes.py from fastapi import APIRouter, HTTPException, Body from typing import Dict, Any, Optional from pydantic import BaseModel from event_dispatcher import EventDispatcher router = APIRouter() event_dispatcher = EventDispatcher() # Event subscription model class EventSubscription(BaseModel): workflow_id: str config: Optional[Dict[str, Any]] = None @router.post(\"/events/subscribe/{event_type}\") async def subscribe_to_event(event_type: str, subscription: EventSubscription): \"\"\"Subscribe a workflow to an event type.\"\"\" result = event_dispatcher.subscribe( event_type, subscription.workflow_id, subscription.config ) return { \"message\": f\"Workflow {subscription.workflow_id} subscribed to event {event_type}\", \"subscription\": result } @router.delete(\"/events/unsubscribe/{event_type}\") async def unsubscribe_from_event(event_type: str, workflow_id: str): \"\"\"Unsubscribe a workflow from an event type.\"\"\" success = event_dispatcher.unsubscribe(event_type, workflow_id) if not success: raise HTTPException(status_code=404, detail=f\"No subscription found for event {event_type} and workflow {workflow_id}\") return { \"message\": f\"Workflow {workflow_id} unsubscribed from event {event_type}\" } @router.get(\"/events\") async def list_events(): \"\"\"List all event subscriptions.\"\"\" return event_dispatcher.get_subscriptions() @router.get(\"/events/{event_type}\") async def get_event_subscriptions(event_type: str): \"\"\"Get subscriptions for a specific event type.\"\"\" subscriptions = event_dispatcher.get_subscriptions(event_type) if not subscriptions.get(event_type): raise HTTPException(status_code=404, detail=f\"No subscriptions found for event {event_type}\") return subscriptions @router.post(\"/events/trigger/{event_type}\") async def trigger_event(event_type: str, payload: Optional[Dict[str, Any]] = Body(None)): \"\"\"Trigger an event, executing all subscribed workflows.\"\"\" results = await event_dispatcher.dispatch(event_type, payload) return { \"message\": f\"Event {event_type} triggered\", \"workflows_executed\": len(results), \"results\": results } . ",
    "url": "/Advanced/Event_Driven.html#adding-event-routes-to-fastapi",
    
    "relUrl": "/Advanced/Event_Driven.html#adding-event-routes-to-fastapi"
  },"33": {
    "doc": "Event-Driven Workflows",
    "title": "Creating Webhook Triggers with Verification",
    "content": "For secure communication with external systems, let’s add webhook support with signature verification: . # webhook_routes.py from fastapi import APIRouter, HTTPException, Request, Header, Depends import json import hmac import hashlib import os from event_dispatcher import EventDispatcher router = APIRouter() event_dispatcher = EventDispatcher() # Get webhook secret from environment (should be properly managed in production) WEBHOOK_SECRET = os.environ.get(\"WEBHOOK_SECRET\", \"change-me-in-production\") async def verify_webhook_signature( request: Request, x_signature: Optional[str] = Header(None, alias=\"X-Webhook-Signature\") ): \"\"\"Verify the webhook signature for authenticity.\"\"\" if not x_signature: # You may want to make this mandatory in production return True # Get request body for signature validation body = await request.body() # Compute expected signature expected_signature = hmac.new( WEBHOOK_SECRET.encode(), body, hashlib.sha256 ).hexdigest() # Verify using constant-time comparison if not hmac.compare_digest(expected_signature, x_signature): raise HTTPException(status_code=401, detail=\"Invalid webhook signature\") return True @router.post(\"/webhooks/{event_type}\") async def webhook_handler( event_type: str, request: Request, verified: bool = Depends(verify_webhook_signature) ): \"\"\"Handle webhook calls for specific event types with signature verification.\"\"\" try: # Parse the payload body = await request.body() if not body: payload = {} else: try: payload = json.loads(body) except json.JSONDecodeError: payload = {\"raw_body\": body.decode(\"utf-8\", errors=\"replace\")} # Add headers and query params to payload payload[\"headers\"] = dict(request.headers) payload[\"query_params\"] = dict(request.query_params) # Dispatch the event results = await event_dispatcher.dispatch(event_type, payload) return { \"status\": \"success\", \"event_type\": event_type, \"workflows_triggered\": len(results) } except Exception as e: raise HTTPException(status_code=500, detail=f\"Webhook processing failed: {str(e)}\") . ",
    "url": "/Advanced/Event_Driven.html#creating-webhook-triggers-with-verification",
    
    "relUrl": "/Advanced/Event_Driven.html#creating-webhook-triggers-with-verification"
  },"34": {
    "doc": "Event-Driven Workflows",
    "title": "Scheduled Events with Background Tasks",
    "content": "Want to trigger workflows on a schedule? Let’s use FastAPI’s background tasks: . # scheduler.py from fastapi import FastAPI, BackgroundTasks, HTTPException, Body import asyncio from datetime import datetime from typing import Dict, Any, Optional from event_dispatcher import EventDispatcher app = FastAPI() event_dispatcher = EventDispatcher() scheduled_tasks = {} async def run_scheduled_event(event_type, payload, interval_seconds): \"\"\"Run an event on a schedule.\"\"\" while True: try: await event_dispatcher.dispatch(event_type, payload) # Update payload with last_run timestamp if isinstance(payload, dict): payload[\"last_run\"] = datetime.now().isoformat() except Exception as e: print(f\"Error in scheduled event {event_type}: {str(e)}\") await asyncio.sleep(interval_seconds) @app.post(\"/schedule/{event_type}\") async def schedule_event( event_type: str, interval_seconds: int, background_tasks: BackgroundTasks, payload: Optional[Dict[str, Any]] = Body(None) ): \"\"\"Schedule an event to run periodically.\"\"\" if event_type in scheduled_tasks: return {\"message\": f\"Event {event_type} already scheduled\"} if interval_seconds &lt; 5: # Reasonable minimum in production raise HTTPException(status_code=400, detail=\"Interval must be at least 5 seconds\") # Initialize payload if None actual_payload = payload or {} actual_payload[\"scheduled_at\"] = datetime.now().isoformat() task = asyncio.create_task(run_scheduled_event(event_type, actual_payload, interval_seconds)) scheduled_tasks[event_type] = { \"task\": task, \"interval\": interval_seconds, \"payload\": actual_payload, \"started_at\": datetime.now().isoformat() } return { \"message\": f\"Event {event_type} scheduled to run every {interval_seconds} seconds\", \"event_type\": event_type, \"interval_seconds\": interval_seconds } @app.delete(\"/schedule/{event_type}\") async def cancel_scheduled_event(event_type: str): \"\"\"Cancel a scheduled event.\"\"\" if event_type not in scheduled_tasks: raise HTTPException(status_code=404, detail=f\"No scheduled event found for {event_type}\") task_info = scheduled_tasks[event_type] task_info[\"task\"].cancel() del scheduled_tasks[event_type] return { \"message\": f\"Scheduled event {event_type} cancelled\", \"event_type\": event_type } @app.get(\"/schedule\") async def list_scheduled_events(): \"\"\"List all scheduled events.\"\"\" result = {} for event_type, task_info in scheduled_tasks.items(): result[event_type] = { \"interval_seconds\": task_info[\"interval\"], \"started_at\": task_info[\"started_at\"], \"payload\": task_info[\"payload\"] } return result . ",
    "url": "/Advanced/Event_Driven.html#scheduled-events-with-background-tasks",
    
    "relUrl": "/Advanced/Event_Driven.html#scheduled-events-with-background-tasks"
  },"35": {
    "doc": "Event-Driven Workflows",
    "title": "Using the Event System",
    "content": "Here’s how to put all this together: . # main.py from fastapi import FastAPI from routes import router as api_router from webhook_routes import router as webhook_router from scheduler import app as scheduler_app app = FastAPI() # Regular API routes app.include_router(api_router, prefix=\"/api\") # Webhook routes - no prefix for easier external access app.include_router(webhook_router) # Include scheduler routes app.include_router(scheduler_app, prefix=\"/api\") if __name__ == \"__main__\": import uvicorn uvicorn.run(app, host=\"0.0.0.0\", port=8000) . ",
    "url": "/Advanced/Event_Driven.html#using-the-event-system",
    
    "relUrl": "/Advanced/Event_Driven.html#using-the-event-system"
  },"36": {
    "doc": "Event-Driven Workflows",
    "title": "Practical Examples",
    "content": "Example 1: Responding to a Payment Event . Subscribe to payment events: . import requests # Register a workflow to process orders when payment is received response = requests.post( \"https://your-api.com/api/events/subscribe/payment_received\", json={ \"workflow_id\": \"order_processing\", \"config\": { \"priority\": \"high\", \"notify_on_completion\": True } } ) . Trigger a payment event: . import requests # Trigger the event when payment is confirmed response = requests.post( \"https://your-api.com/api/events/trigger/payment_received\", json={ \"order_id\": \"12345\", \"amount\": 99.99, \"currency\": \"USD\", \"payment_method\": \"credit_card\" } ) . Example 2: Setting Up a GitHub Webhook with Signature Verification . | Generate a secure webhook secret and configure it in your environment: export WEBHOOK_SECRET=your-secure-secret-key . | Configure your GitHub repository webhook settings: . | URL: https://your-api.com/webhooks/github_push | Content type: application/json | Secret: Same as your WEBHOOK_SECRET | Events: Select “Push” events | . | Subscribe your workflow: import requests response = requests.post( \"https://your-api.com/api/events/subscribe/github_push\", json={ \"workflow_id\": \"auto_deploy\", \"config\": { \"branches\": [\"main\", \"production\"], \"deploy_target\": \"production\" } } ) . | Verify your webhook is working: import requests response = requests.get( \"https://your-api.com/webhooks/github_push/test\" ) . | . Example 3: Scheduling a Daily Report . import requests # Schedule a report generation event to run daily response = requests.post( \"https://your-api.com/api/schedule/generate_daily_report\", json={ \"interval_seconds\": 86400, # 24 hours \"payload\": { \"report_type\": \"sales_summary\", \"recipients\": [\"team@company.com\"] } } ) . ",
    "url": "/Advanced/Event_Driven.html#practical-examples",
    
    "relUrl": "/Advanced/Event_Driven.html#practical-examples"
  },"37": {
    "doc": "Event-Driven Workflows",
    "title": "Wrapping Up",
    "content": "With this event system, your workflows can spring to life at just the right moment. The persistent storage ensures your event subscriptions survive system restarts, while signature verification keeps your webhooks secure. Whether it’s responding to API calls, webhooks from external services, or running on a schedule, you’ve got the tools to make your state machine truly event-driven. Remember, the best automation is the kind you don’t have to think about - it just happens when it should! . ",
    "url": "/Advanced/Event_Driven.html#wrapping-up",
    
    "relUrl": "/Advanced/Event_Driven.html#wrapping-up"
  },"38": {
    "doc": "Event-Driven Workflows",
    "title": "Event-Driven Workflows",
    "content": " ",
    "url": "/Advanced/Event_Driven.html",
    
    "relUrl": "/Advanced/Event_Driven.html"
  },"39": {
    "doc": "Extending Logging System",
    "title": "Extending the Logging System",
    "content": " ",
    "url": "/Advanced/Extending_Logging.html#extending-the-logging-system",
    
    "relUrl": "/Advanced/Extending_Logging.html#extending-the-logging-system"
  },"40": {
    "doc": "Extending Logging System",
    "title": "Where Do Your Workflows Go When They Sleep?",
    "content": "By default, Grapheteria stores all your workflow logs in the local filesystem. This works great for development, but when you’re ready for the big leagues (aka production), you might want something more robust. Maybe you need centralized storage, better querying capabilities, or just don’t trust those pesky local files (we’ve all accidentally rm -rf‘d something important, right?). ",
    "url": "/Advanced/Extending_Logging.html#where-do-your-workflows-go-when-they-sleep",
    
    "relUrl": "/Advanced/Extending_Logging.html#where-do-your-workflows-go-when-they-sleep"
  },"41": {
    "doc": "Extending Logging System",
    "title": "Tracking Data: The Memory of Your Workflows",
    "content": "Every workflow maintains a tracking_data structure that contains: . { \"workflow_id\": \"your.awesome.workflow\", \"run_id\": \"20230415_120523_123\", \"steps\": [ # State snapshot after each step execution # Each containing everything needed to resume execution ] } . The steps list is particularly magical - it captures the complete execution state after each node runs. This is what enables our workflow to pick up exactly where it left off, even if your server decided to take an unplanned vacation. At the end of every step, the workflow engine calls self.storage.save_state() with the storage object being whatever custom backend you decide to provide (defaults to local file system) . ",
    "url": "/Advanced/Extending_Logging.html#tracking-data-the-memory-of-your-workflows",
    
    "relUrl": "/Advanced/Extending_Logging.html#tracking-data-the-memory-of-your-workflows"
  },"42": {
    "doc": "Extending Logging System",
    "title": "Creating Your Own Storage Backend",
    "content": "The StorageBackend abstract class defines the interface any storage implementation must follow: . class StorageBackend(ABC): @abstractmethod def save_state(self, workflow_id: str, run_id: str, tracking_data: dict) -&gt; None: \"\"\"Save the workflow execution state.\"\"\" pass @abstractmethod def load_state(self, workflow_id: str, run_id: str) -&gt; Optional[Dict]: \"\"\"Load a workflow execution state.\"\"\" pass @abstractmethod def list_runs(self, workflow_id: str) -&gt; List[str]: \"\"\"List all runs for a given workflow.\"\"\" pass @abstractmethod def list_workflows(self) -&gt; List[str]: \"\"\"List all workflows.\"\"\" pass . Let’s break down these methods: . | save_state: Stores the workflow state with its unique identifiers | load_state: Retrieves the state of a specific workflow run | list_runs: Gets all runs for a specific workflow (useful for history/debugging) | list_workflows: Lists all available workflows in the storage | . ",
    "url": "/Advanced/Extending_Logging.html#creating-your-own-storage-backend",
    
    "relUrl": "/Advanced/Extending_Logging.html#creating-your-own-storage-backend"
  },"43": {
    "doc": "Extending Logging System",
    "title": "Example: SQLite Storage Implementation",
    "content": "Here’s an example of implementing SQLite storage - observe how it satisfies the same interface: . from grapheteria.utils import StorageBackend class SQLiteStorage(StorageBackend): def __init__(self, db_path: str = \"workflows.db\"): self.db_path = db_path self._init_db() def _init_db(self): # Creates the database table if it doesn't exist with self._get_connection() as conn: cursor = conn.cursor() cursor.execute(''' CREATE TABLE IF NOT EXISTS workflow_states ( workflow_id TEXT, run_id TEXT, state_json TEXT, updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (workflow_id, run_id) ) ''') conn.commit() @contextmanager def _get_connection(self): conn = sqlite3.connect(self.db_path) try: yield conn finally: conn.close() def save_state(self, workflow_id: str, run_id: str, source_data: dict) -&gt; None: # Saves workflow state as JSON in the database with self._get_connection() as conn: cursor = conn.cursor() cursor.execute( ''' INSERT OR REPLACE INTO workflow_states (workflow_id, run_id, state_json, updated_at) VALUES (?, ?, ?, CURRENT_TIMESTAMP) ''', (workflow_id, run_id, json.dumps(source_data)) ) conn.commit() def load_state(self, workflow_id: str, run_id: str) -&gt; Optional[Dict]: # Retrieves workflow state from the database with self._get_connection() as conn: cursor = conn.cursor() cursor.execute( \"SELECT state_json FROM workflow_states WHERE workflow_id = ? AND run_id = ?\", (workflow_id, run_id) ) row = cursor.fetchone() if not row: return None return json.loads(row[0]) . Using the SQLite backend is as simple as: . # Initialize the workflow engine with SQLite storage storage = SQLiteStorage(\"production.db\") engine = WorkflowEngine( workflow_id=\"my.workflow\", storage_backend=storage ) # Now all state is stored in SQLite! . ",
    "url": "/Advanced/Extending_Logging.html#example-sqlite-storage-implementation",
    
    "relUrl": "/Advanced/Extending_Logging.html#example-sqlite-storage-implementation"
  },"44": {
    "doc": "Extending Logging System",
    "title": "Creating Your Own Storage",
    "content": "Want to store workflows in Redis, MongoDB, or your secret underground bunker’s mainframe? Just implement those four methods and you’re good to go! . Here’s a stub for your next storage adventure: . class MyAwesomeStorage(StorageBackend): def __init__(self, connection_string): # Connect to your storage service self.client = AwesomeStorageClient(connection_string) def save_state(self, workflow_id, run_id, source_data): # Your code to save state self.client.upsert_document( collection=\"workflows\", key=f\"{workflow_id}:{run_id}\", data=source_data ) # Implement the other required methods... Now your workflows can live wherever they want - give them the freedom they deserve! . ",
    "url": "/Advanced/Extending_Logging.html#creating-your-own-storage",
    
    "relUrl": "/Advanced/Extending_Logging.html#creating-your-own-storage"
  },"45": {
    "doc": "Extending Logging System",
    "title": "Extending Logging System",
    "content": " ",
    "url": "/Advanced/Extending_Logging.html",
    
    "relUrl": "/Advanced/Extending_Logging.html"
  },"46": {
    "doc": "Human-in-the-Loop",
    "title": "Human-in-the-Loop: Halting and Resuming Workflows",
    "content": " ",
    "url": "/Advanced/Human_in_the_loop.html#human-in-the-loop-halting-and-resuming-workflows",
    
    "relUrl": "/Advanced/Human_in_the_loop.html#human-in-the-loop-halting-and-resuming-workflows"
  },"47": {
    "doc": "Human-in-the-Loop",
    "title": "The Need for Human Input",
    "content": "In agentic workflows, we often need humans to validate, correct, or provide data that machines can’t determine. Like asking your GPS if you should take that questionable shortcut through what appears to be someone’s backyard - sometimes human judgment is irreplaceable. Our state machine provides a built-in mechanism for pausing execution, collecting human input, and seamlessly resuming - whether immediately or days later. ",
    "url": "/Advanced/Human_in_the_loop.html#the-need-for-human-input",
    
    "relUrl": "/Advanced/Human_in_the_loop.html#the-need-for-human-input"
  },"48": {
    "doc": "Human-in-the-Loop",
    "title": "How It Works: Futures and States",
    "content": "The system uses Python’s asyncio.Future objects to implement halting. When a node requests input: . user_name = await request_input(prompt=\"What's your name?\") print(f\"Hello, {user_name}!\") . Behind the scenes: . | The workflow status changes to WAITING_FOR_INPUT | The current state is saved | A future is created and awaited, pausing execution | When input arrives, the future resolves and execution continues | . Same process resumption is quite tricky to get right. An await operation on the asyncio.Future object blocks python’s event loop, causing your workflow to become seemingly unresponsive. In reality, it is currently awaiting and this is intended behavior. To design a seamless system, use your workflow in tandem with a server. This way python switches between coroutines and you can provide inputs with ease. ",
    "url": "/Advanced/Human_in_the_loop.html#how-it-works-futures-and-states",
    
    "relUrl": "/Advanced/Human_in_the_loop.html#how-it-works-futures-and-states"
  },"49": {
    "doc": "Human-in-the-Loop",
    "title": "Resumption Behavior",
    "content": "There are two ways execution can resume after requesting input: . Same-Process Resumption . If input arrives while the original process is still running: . # In your workflow node data = await request_input(prompt=\"Enter data:\") process_data(data) # Continues from exactly here . Cross-Process Resumption . If the workflow is stopped and later restarted: . # First execution data = await request_input(prompt=\"Enter data:\") # Halts here # Days later, workflow engine restarts data = await request_input(prompt=\"Enter data:\") # Re-executes this line process_data(data) # Then continues . ⚠️ Important: Code before the await will execute twice in cross-process resumption. Avoid side effects before await points: . # BAD: Side effect before await send_notification() # Will run twice if resumed cross-process data = await request_input(prompt=\"Enter data:\") # GOOD: Side effect after await data = await request_input(prompt=\"Enter data:\") send_notification() # Will only run once . ",
    "url": "/Advanced/Human_in_the_loop.html#resumption-behavior",
    
    "relUrl": "/Advanced/Human_in_the_loop.html#resumption-behavior"
  },"50": {
    "doc": "Human-in-the-Loop",
    "title": "Request Input Parameters",
    "content": "The request_input function accepts these parameters: . async def request_input(prompt=None, options=None, input_type=\"text\", request_id=None): . | prompt: Text shown to the user (e.g., “What’s your name?”) | options: Available choices for selection inputs | input_type: Format of input (“text”, “select”, etc.) | request_id: Custom identifier for this specific input request | . The request_id is critical when a node has multiple input requests: . name = await request_input(prompt=\"Name?\", request_id=\"name_field\") age = await request_input(prompt=\"Age?\", request_id=\"age_field\") . Without unique request_ids, the same input would satisfy both requests! . ",
    "url": "/Advanced/Human_in_the_loop.html#request-input-parameters",
    
    "relUrl": "/Advanced/Human_in_the_loop.html#request-input-parameters"
  },"51": {
    "doc": "Human-in-the-Loop",
    "title": "Providing Input Data",
    "content": "To resume a workflow, provide input as a dictionary to either the step() or run() methods of the workflow engine: . # For inputs with default node IDs input_data = { \"TextInputNode_12345\": \"User's response\" } # For inputs with custom request IDs input_data = { \"name_field\": \"Alice\", \"age_field\": 30 } # Complex input data is also supported input_data = { \"form_response\": { \"name\": \"Bob\", \"preferences\": [\"pizza\", \"hiking\"] } } # Resume workflow with inputs await workflow.step(input_data) . The key in the dictionary must match either the node ID or the custom request ID. The respective value is the actual input data you wish to provide to the node. ",
    "url": "/Advanced/Human_in_the_loop.html#providing-input-data",
    
    "relUrl": "/Advanced/Human_in_the_loop.html#providing-input-data"
  },"52": {
    "doc": "Human-in-the-Loop",
    "title": "Input Consumption Behavior",
    "content": "When a workflow is halted: . | You must call workflow.step() or workflow.run() with input data to resume | Inputs are consumed when used and won’t persist for future requests | If inputs are provided for the wrong halted node, the workflow remains halted | Inputs provided in advance will be lost if not immediately used | . # This will NOT work: await workflow.step({\"future_node_id\": \"data\"}) # Input for a node not yet halted # ... later when the node halts ... await workflow.step() # The input is already gone! . ",
    "url": "/Advanced/Human_in_the_loop.html#input-consumption-behavior",
    
    "relUrl": "/Advanced/Human_in_the_loop.html#input-consumption-behavior"
  },"53": {
    "doc": "Human-in-the-Loop",
    "title": "Example of an Agent Requiring Feedback",
    "content": "class ContentReviewNode(Node): def prepare(self, shared, request_input): return { \"content\": shared.get(\"draft_content\", \"\"), \"request_input\": request_input, \"llm_client\": shared.get(\"llm_client\") } async def execute(self, prepared_data): request_input = prepared_data[\"request_input\"] content = prepared_data[\"content\"] llm_client = prepared_data[\"llm_client\"] # First, get LLM suggestions prompt = f\"Suggest improvements for this content:\\n\\n{content}\" llm_response = await llm_client.generate(prompt) # Show suggestions to human for approval user_decision = await request_input( prompt=\"The AI suggests these improvements. Accept?\", options=[\"Accept All\", \"Accept Some\", \"Reject All\"], input_type=\"select\", request_id=\"improvement_decision\" ) if user_decision == \"Accept Some\": # Request specific edits from human specific_edits = await request_input( prompt=\"Which specific changes would you like to make?\", request_id=\"specific_edits\" ) # Second LLM call with human guidance refined_prompt = f\"Revise this content:\\n\\n{content}\\n\\nWith these specific changes: {specific_edits}\" final_content = await llm_client.generate(refined_prompt) elif user_decision == \"Accept All\": final_content = llm_response else: final_content = content return {\"original\": content, \"final\": final_content} def cleanup(self, shared, prepared_data, execution_result): shared[\"reviewed_content\"] = execution_result[\"final\"] . ",
    "url": "/Advanced/Human_in_the_loop.html#example-of-an-agent-requiring-feedback",
    
    "relUrl": "/Advanced/Human_in_the_loop.html#example-of-an-agent-requiring-feedback"
  },"54": {
    "doc": "Human-in-the-Loop",
    "title": "Human-in-the-Loop",
    "content": " ",
    "url": "/Advanced/Human_in_the_loop.html",
    
    "relUrl": "/Advanced/Human_in_the_loop.html"
  },"55": {
    "doc": "Logging",
    "title": "Persistence and Time Travel",
    "content": " ",
    "url": "/Core/Logging.html#persistence-and-time-travel",
    
    "relUrl": "/Core/Logging.html#persistence-and-time-travel"
  },"56": {
    "doc": "Logging",
    "title": "Overview",
    "content": "You’ve already met the building blocks of our workflow engine - nodes, edges, shared variables, and the engine itself. But there’s more to this story! Our workflow engine doesn’t just execute steps and forget them. It’s like a time traveler with a detailed journal - recording every step, allowing you to pause journeys, resume them later, or even create alternate timelines. Let’s see how this magic works. ",
    "url": "/Core/Logging.html#overview",
    
    "relUrl": "/Core/Logging.html#overview"
  },"57": {
    "doc": "Logging",
    "title": "Run IDs: Your Workflow’s Passport",
    "content": "When a workflow starts its journey, it gets a unique passport - a run ID: . engine = WorkflowEngine( nodes=[start_node, process_node, end_node], start=start_node ) # A unique run_id like '20230615_143022_789' is auto-generated print(f\"Remember this ID to resume later: {engine.run_id}\") . This ID is your ticket back to this exact workflow state. Store it somewhere safe if you want to return to this journey later! . By default, logs are stored in the logs directory of your current working directory. You can examine these logs anytime to find historical run_ids. For a more user-friendly experience, all logs are also accessible through the UI where they’re formatted and easier to navigate. ",
    "url": "/Core/Logging.html#run-ids-your-workflows-passport",
    
    "relUrl": "/Core/Logging.html#run-ids-your-workflows-passport"
  },"58": {
    "doc": "Logging",
    "title": "Resuming Workflows: Pick Up Where You Left Off",
    "content": "Life happens. Servers restart. But your workflow can continue right where it paused: . # Resume the workflow from the most recent step resumed_engine = WorkflowEngine( workflow_id=\"my_awesome_workflow\", run_id=\"20230615_143022_789\" ) # Or resume from a specific step resumed_engine = WorkflowEngine( workflow_id=\"my_awesome_workflow\", run_id=\"20230615_143022_789\", resume_from=3 # Resume from step 3 ) . The engine automatically loads the state and prepares to continue execution - whether it was waiting for input or ready to process the next node. ",
    "url": "/Core/Logging.html#resuming-workflows-pick-up-where-you-left-off",
    
    "relUrl": "/Core/Logging.html#resuming-workflows-pick-up-where-you-left-off"
  },"59": {
    "doc": "Logging",
    "title": "Forking Workflows: Creating Alternate Timelines",
    "content": "Want to experiment with different paths without losing your original journey? Fork it! . # Create a new branch from step 3 of a previous run forked_engine = WorkflowEngine( workflow_id=\"my_awesome_workflow\", run_id=\"20230615_143022_789\", resume_from=3, fork=True # This creates a new run_id and preserves the original ) . This creates a parallel universe - your original workflow remains intact while you explore a different path from the same starting point. ",
    "url": "/Core/Logging.html#forking-workflows-creating-alternate-timelines",
    
    "relUrl": "/Core/Logging.html#forking-workflows-creating-alternate-timelines"
  },"60": {
    "doc": "Logging",
    "title": "State Validation: Keeping Your Timeline Consistent",
    "content": "Time travel can be messy. To prevent paradoxes, the engine validates that your current workflow definition is compatible with the saved state: . try: # This will fail if 'critical_node' from step 5 is missing resumed_engine = WorkflowEngine( workflow_id=\"my_awesome_workflow\", run_id=\"20230615_143022_789\", resume_from=5 ) except ValueError as e: print(f\"Can't resume: {e}\") # \"Cannot resume: Node 'critical_node' is missing...\" . You can add new nodes to the future, but you can’t erase the past - nodes that were already processed or waiting must exist in your current workflow. ",
    "url": "/Core/Logging.html#state-validation-keeping-your-timeline-consistent",
    
    "relUrl": "/Core/Logging.html#state-validation-keeping-your-timeline-consistent"
  },"61": {
    "doc": "Logging",
    "title": "Storage Configuration: Beyond Local Files",
    "content": "By default, your workflow’s history is stored in the local filesystem - perfect for development: . # Default storage uses local filesystem engine = WorkflowEngine(nodes=[...]) # For production, configure a different storage backend from storage_backend import PostgresStorage engine = WorkflowEngine( nodes=[...], storage_backend=PostgresStorage(connection_string=\"postgresql://...\") ) . For more robust production environments, check out our Storage Configuration guide for options like database storage, cloud storage, and more. Now you’re ready to build workflows that can pause, resume, and even branch into different timelines. Happy time traveling! . ",
    "url": "/Core/Logging.html#storage-configuration-beyond-local-files",
    
    "relUrl": "/Core/Logging.html#storage-configuration-beyond-local-files"
  },"62": {
    "doc": "Logging",
    "title": "Logging",
    "content": " ",
    "url": "/Core/Logging.html",
    
    "relUrl": "/Core/Logging.html"
  },"63": {
    "doc": "Logs Page",
    "title": "Logs",
    "content": " ",
    "url": "/Core/UI/Logs.html#logs",
    
    "relUrl": "/Core/UI/Logs.html#logs"
  },"64": {
    "doc": "Logs Page",
    "title": "Overview",
    "content": "The logs tab is your time machine for workflows. It lets you peek into the past, present, and alternate timelines of every workflow you’ve created. Here you can track every step, decision, and path your workflows have taken throughout their journey. Currently, logs are stored on your local filesystem (we’re working on fancy database options soon, we promise!). ",
    "url": "/Core/UI/Logs.html#overview",
    
    "relUrl": "/Core/UI/Logs.html#overview"
  },"65": {
    "doc": "Logs Page",
    "title": "Selecting Workflows",
    "content": "Finding the workflow you’re looking for is a breeze. Simply: . | Navigate to the logs tab | Browse the list of available workflows | Click on any workflow that catches your eye | . Each workflow displays its run history - a collection of timestamps that serve as both run IDs and breadcrumbs showing when each execution occurred. ",
    "url": "/Core/UI/Logs.html#selecting-workflows",
    
    "relUrl": "/Core/UI/Logs.html#selecting-workflows"
  },"66": {
    "doc": "Logs Page",
    "title": "Exploring Run Details",
    "content": "Clicked on a run and ready to dive deeper? Each run reveals its secrets: . | Workflow ID | Run ID | Detailed execution steps | Previous node ID | Next node ID | Awaiting nodes | …and much more fascinating data! | . Think of it as a workflow’s diary - recording every thought, action, and decision it made during its execution. ",
    "url": "/Core/UI/Logs.html#exploring-run-details",
    
    "relUrl": "/Core/UI/Logs.html#exploring-run-details"
  },"67": {
    "doc": "Logs Page",
    "title": "Tracking Workflow Evolution",
    "content": "Sometimes workflows branch into new variations - like alternate timelines in a sci-fi movie. When a workflow was forked from a previous run, you’ll see: . | A clickable link to the parent run at the top | Detailed metadata showing exactly where and how the fork happened | . This feature is particularly handy when you’re experimenting with different workflow paths or debugging complex processes. Ready to become a workflow time traveler? The logs tab awaits your exploration! . ",
    "url": "/Core/UI/Logs.html#tracking-workflow-evolution",
    
    "relUrl": "/Core/UI/Logs.html#tracking-workflow-evolution"
  },"68": {
    "doc": "Logs Page",
    "title": "Logs Page",
    "content": " ",
    "url": "/Core/UI/Logs.html",
    
    "relUrl": "/Core/UI/Logs.html"
  },"69": {
    "doc": "Node",
    "title": "Working with Nodes",
    "content": " ",
    "url": "/Core/Node.html#working-with-nodes",
    
    "relUrl": "/Core/Node.html#working-with-nodes"
  },"70": {
    "doc": "Node",
    "title": "Overview",
    "content": "The Node class is the smallest unit of execution in Grapheteria’s workflow system - think of it as the atom in your workflow molecule. All task-performing classes must extend this class to join the party. Nodes handle individual pieces of work, process data, make decisions, or interact with external systems. from grapheteria import Node class MyCustomNode(Node): def execute(self, prepared_result): # Your execution logic goes here return \"Hello, Grapheteria!\" . ",
    "url": "/Core/Node.html#overview",
    
    "relUrl": "/Core/Node.html#overview"
  },"71": {
    "doc": "Node",
    "title": "The Triple-Phase Model",
    "content": "Grapheteria nodes follow a clear three-phase execution model inspired by PocketFlow, bringing order to the potentially chaotic world of workflow execution. This separation creates clear boundaries for different responsibilities and improves maintainability. 1. Prepare . The prepare function sets the stage for execution. It receives two parameters: . | shared: The shared state dictionary for cross-node communication | request_input: A function that can request for human input | . # Use this function to read from shared, a db or pre-process data def prepare(self, shared, request_input): # Extract what you need from shared state name = shared.get(\"user_name\", \"friend\") initial_data = shared.get(\"data\", {}) # Return exactly what execute needs - nothing more, nothing less return {\"name\": name, \"greeting\": \"Hello\", \"data\": initial_data} . 2. Execute . The execute function is where the magic happens. It receives only one parameter: . | prepared_result: The output from the prepare phase | . # The main work happens here, using only what prepare provided. Call an API or perform computation. def execute(self, prepared_result): processed_data = do_something_with(prepared_result[\"data\"]) return { \"message\": f\"{prepared_result['greeting']}, {prepared_result['name']}!\", \"processed_data\": processed_data } . Notice how execute doesn’t receive the shared state directly. This is intentional! It: . | Prevents accidental corruption of shared state during critical operations | Enables future parallel execution of multiple nodes (see Parallelism docs) | Forces clean separation of concerns between phases | . Execution comes with built-in resilience: . | max_retries: Number of attempts before giving up (default: 1) | wait: Time to wait between retries in seconds | exec_fallback: Method called when all retries fail | . class ReliableNode(Node): async def execute(self, prepared_result): # Potentially flaky operation await call_external_api() return def exec_fallback(self, prepared_result, exception): # Handle the failure gracefully return {\"status\": \"failed\", \"reason\": str(exception)} # Create instance with retry parameters reliable_node = ReliableNode(id=\"reliable\", max_retries=3, wait=2) . 3. Cleanup . The cleanup function handles post-execution tasks. It receives all three pieces of context: . | shared: The shared state dictionary | prepared_result: The original output from prepare | execution_result: The output from execute | . # Update shared state with our results or write results to a db def cleanup(self, shared, prepared_result, execution_result): shared[\"greeting_message\"] = execution_result[\"message\"] shared[\"processed_data\"] = execution_result[\"processed_data\"] # write_to_db() . ",
    "url": "/Core/Node.html#the-triple-phase-model",
    
    "relUrl": "/Core/Node.html#the-triple-phase-model"
  },"72": {
    "doc": "Node",
    "title": "Custom Node IDs",
    "content": "Always define a custom ID for each node rather than relying on auto-generated IDs: . # Good: Descriptive, unique ID node = MyCustomNode(id=\"validate_user_input_step\") # Bad: Relying on auto-generated ID node = MyCustomNode() # Gets something like \"MyCustomNode_a1b2c3d4\" . Custom IDs are crucial for: . | Logging and debugging - imagine searching logs for “validate_user_input_step” vs “MyCustomNode_a1b2c3d4” | Resuming workflows after interruption - when restarting a workflow, the system needs to know exactly which node to resume from | Providing data to halted nodes requesting human input - when a node is waiting for input, you need a clear ID to send that input to the right place | . Without meaningful IDs, your workflow becomes a mysterious black box. With them, it transforms into a transparent, manageable, and resumable process. ",
    "url": "/Core/Node.html#custom-node-ids",
    
    "relUrl": "/Core/Node.html#custom-node-ids"
  },"73": {
    "doc": "Node",
    "title": "Node Configuration",
    "content": "Nodes can be configured through a config dictionary passed during initialization. This enhances reusability - the same node class can be used for multiple purposes just by changing its configuration. # Create two different LLM agents from the same class customer_service = LLMNode(id = \"customer_service\", config={ \"system_prompt\": \"You are a helpful customer service representative.\", \"temperature\": 0.3, \"max_tokens\": 500 }) creative_writer = LLMNode(id=\"creative_writer\", config={ \"system_prompt\": \"You are a creative storyteller with a flair for drama.\", \"temperature\": 0.9, \"max_tokens\": 2000 }) . Access config values inside your node methods: . def prepare(self, shared, request_input): system_prompt = self.config.get(\"system_prompt\", \"Default system prompt\") temperature = self.config.get(\"temperature\", 0.7) return { \"system_message\": system_prompt, \"prompt\": shared.get(\"user_message\", \"\"), \"temperature\": temperature } . ",
    "url": "/Core/Node.html#node-configuration",
    
    "relUrl": "/Core/Node.html#node-configuration"
  },"74": {
    "doc": "Node",
    "title": "Using request_input",
    "content": "The request_input function allows nodes to request external input during execution - perfect for human-in-the-loop scenarios. The function can be called without any parameters, though additional information helps guide the user: . async def prepare(self, shared, request_input): # Simple confirmation prompt - with helpful parameters user_choice = await request_input( prompt=\"Do you approve this transaction?\", options=[\"Approve\", \"Reject\"], input_type=\"select\" ) # If rejection, ask for reason in the same prepare phase if user_choice == \"Reject\": reason = await request_input( prompt=\"Please provide reason for rejection:\", input_type=\"text\", request_id=\"rejection_reason\" # Different from default node ID ) return {\"status\": \"rejected\", \"reason\": reason} # Store the choice for execute phase return {\"user_approved\": True} . The request_id parameter differentiates between multiple input requests within the same node. Without it, the same input would be reused for all calls (defaulting to the node’s ID). For a more informative lesson on request_input() please check out the Human-in-the-Loop docs. ",
    "url": "/Core/Node.html#using-request_input",
    
    "relUrl": "/Core/Node.html#using-request_input"
  },"75": {
    "doc": "Node",
    "title": "Running Nodes Standalone",
    "content": "For testing and debugging, you can run nodes independently without setting up an entire workflow: . import asyncio async def test_node(): # Create initial shared state shared_state = {\"user_name\": \"Grapheteria Fan\", \"data\": {\"key\": \"value\"}} # Create and run the node node = ProcessingNode(config={\"processing_level\": \"detailed\"}) result = await node.run_standalone(shared_state) print(f\"Updated shared state: {result}\") print(f\"Processed data: {result.get('processed_data')}\") # Run it asyncio.run(test_node()) . Note that request_input functionality won’t work in standalone mode - it’s strictly for testing node logic without human interaction. ",
    "url": "/Core/Node.html#running-nodes-standalone",
    
    "relUrl": "/Core/Node.html#running-nodes-standalone"
  },"76": {
    "doc": "Node",
    "title": "Initializing Nodes in JSON and Code",
    "content": "Grapheteria offers flexibility by letting you define workflows in both Python code and JSON. While your Node class implementation must be in Python, you can instantiate and connect nodes using either approach. In Code (Python) . # Create a processing node with a custom ID and configuration processor = MyCustomNode( id=\"data_processor_1\", config={\"max_items\": 100, \"verbose\": True} ) . In JSON . { \"nodes\": [ { \"id\": \"data_processor_1\", \"class\": \"MyCustomNode\", \"config\": {\"max_items\": 100, \"verbose\": true} } ] } . Why JSON? JSON workflows sync in real-time with the UI, letting devs design and modify workflows visually with an intuitive debugging experience. With these building blocks, you can create nodes that gracefully handle any workflow task your application needs! . ",
    "url": "/Core/Node.html#initializing-nodes-in-json-and-code",
    
    "relUrl": "/Core/Node.html#initializing-nodes-in-json-and-code"
  },"77": {
    "doc": "Node",
    "title": "Node",
    "content": " ",
    "url": "/Core/Node.html",
    
    "relUrl": "/Core/Node.html"
  },"78": {
    "doc": "Overview",
    "title": "The UI: Visual Workflow Manager",
    "content": " ",
    "url": "/Core/UI/Overview.html#the-ui-visual-workflow-manager",
    
    "relUrl": "/Core/UI/Overview.html#the-ui-visual-workflow-manager"
  },"79": {
    "doc": "Overview",
    "title": "Discovering Your Components",
    "content": "When launched, the server automatically scans your working directory for: . | Any nodes you’ve created (classes extending the Node class) | Existing workflows (stored as JSON files) | . Everything is instantly available in your browser - no manual importing needed! . ",
    "url": "/Core/UI/Overview.html#discovering-your-components",
    
    "relUrl": "/Core/UI/Overview.html#discovering-your-components"
  },"80": {
    "doc": "Overview",
    "title": "Canvas: Your Workflow Playground",
    "content": "After selecting or creating a workflow, you’ll see your canvas - the blank slate for your state machine. Need to add nodes? Just right-click anywhere on the canvas to see all available nodes. Each node you add automatically updates your workflow’s JSON schema. The UI automatically assigns a unique ID to each node in the JSON schema. If you wish to modify them, please ensure that they are unique and that the changes are reflected in the edges as well. ",
    "url": "/Core/UI/Overview.html#canvas-your-workflow-playground",
    
    "relUrl": "/Core/UI/Overview.html#canvas-your-workflow-playground"
  },"81": {
    "doc": "Overview",
    "title": "Building Your State Machine",
    "content": "Adding and Configuring Nodes . Add as many nodes as your workflow needs. Right-click on any node to: . | Set it as the start node | Modify its configuration | View its source code | And more! | . Creating Connections . Connect your nodes by dragging from the center of one node to another. An edge appears, linking them together - and yes, your JSON file updates in real-time! . Removing Components . To delete a node or edge: . | Double-click on a node’s handle (the same handle lets you drag nodes around) | Double-click anywhere on an edge to remove it | . Edge Configuration . Edges aren’t just connections - click the button on any edge to add transition conditions. ",
    "url": "/Core/UI/Overview.html#building-your-state-machine",
    
    "relUrl": "/Core/UI/Overview.html#building-your-state-machine"
  },"82": {
    "doc": "Overview",
    "title": "Setting Initial State",
    "content": "Look for the button at the bottom of your canvas to set your workflow’s initial state - crucial for proper execution! . ",
    "url": "/Core/UI/Overview.html#setting-initial-state",
    
    "relUrl": "/Core/UI/Overview.html#setting-initial-state"
  },"83": {
    "doc": "Overview",
    "title": "Real-Time Synchronization",
    "content": "Keep an eye on the connection icon in the top left. Green means you’re connected and changes are syncing to your JSON file. The UI only shows what’s actually in your schema. If something doesn’t appear as expected, check the connection status - your progress is always safe. ",
    "url": "/Core/UI/Overview.html#real-time-synchronization",
    
    "relUrl": "/Core/UI/Overview.html#real-time-synchronization"
  },"84": {
    "doc": "Overview",
    "title": "Ready to Run",
    "content": "Once your workflow looks good, head to the debug/run tab on the middle right of the screen. There you can test your state machine and see it in action! . Learn more about debugging and running workflows . ",
    "url": "/Core/UI/Overview.html#ready-to-run",
    
    "relUrl": "/Core/UI/Overview.html#ready-to-run"
  },"85": {
    "doc": "Overview",
    "title": "Troubleshooting",
    "content": "Having issues? Check out our troubleshooting guide for common problems and solutions. Troubleshooting Guide . ",
    "url": "/Core/UI/Overview.html#troubleshooting",
    
    "relUrl": "/Core/UI/Overview.html#troubleshooting"
  },"86": {
    "doc": "Overview",
    "title": "Overview",
    "content": " ",
    "url": "/Core/UI/Overview.html",
    
    "relUrl": "/Core/UI/Overview.html"
  },"87": {
    "doc": "Communication",
    "title": "Shared: The Communication Protocol",
    "content": " ",
    "url": "/Core/Shared.html#shared-the-communication-protocol",
    
    "relUrl": "/Core/Shared.html#shared-the-communication-protocol"
  },"88": {
    "doc": "Communication",
    "title": "Overview",
    "content": "The shared dictionary is your workflow’s memory bank - the primary way nodes talk to each other. Think of it as a communal whiteboard where any node can read existing information or write new data. It’s simply a Python dictionary accessible to all nodes in your workflow. def prepare(self, shared, request_input): # Read something from shared state previous_result = shared.get('previous_calculation', 0) # Use that data in calculations return previous_result * 2 def cleanup(self, shared, prepared_result, execution_result): # Write back to shared state for other nodes shared['my_calculation'] = execution_result return execution_result . ",
    "url": "/Core/Shared.html#overview",
    
    "relUrl": "/Core/Shared.html#overview"
  },"89": {
    "doc": "Communication",
    "title": "Setting Initial State",
    "content": "By default, the shared dictionary starts empty ({}), but you can pre-load it with initial values that will evolve during workflow execution: . # When creating a workflow from code initial_shared_state={ \"chat_history\": [], \"processing_results\": [], \"retry_count\": 0, \"last_execution_time\": None } . Remember that the shared dictionary is meant for dynamic values that change as your workflow runs. For fixed inputs, use node configuration parameters instead. Setting the same initial state in JSON which can be used with …. yeah yeah the UI, you get it. { \"nodes\": [\"...\"], \"edges\": [\"...\"], \"initial_state\": { \"user_profile\": { \"name\": \"Alice\", \"preferences\": [\"quick\", \"automated\"] } } } . ",
    "url": "/Core/Shared.html#setting-initial-state",
    
    "relUrl": "/Core/Shared.html#setting-initial-state"
  },"90": {
    "doc": "Communication",
    "title": "Serialization Constraints",
    "content": "Since workflow states are saved to disk, variables in your shared dictionary must be JSON-serializable by default. This includes: . | Simple types: strings, numbers, booleans, None | Containers: lists, dictionaries | Nested combinations of the above | . # ✅ This works fine shared[\"results\"] = [1, 2, 3] shared[\"config\"] = {\"max_retries\": 3, \"enabled\": True} # ❌ This will cause errors during state saving shared[\"queue\"] = queue.Queue() # Not JSON serializable shared[\"model\"] = sklearn.linear_model.LinearRegression() # Not serializable . Need to store complex Python objects? You’ll need to extend the storage backend to use pickle or another serialization method. See our Extending Storage guide for the full details. ",
    "url": "/Core/Shared.html#serialization-constraints",
    
    "relUrl": "/Core/Shared.html#serialization-constraints"
  },"91": {
    "doc": "Communication",
    "title": "Communication",
    "content": " ",
    "url": "/Core/Shared.html",
    
    "relUrl": "/Core/Shared.html"
  },"92": {
    "doc": "Troubleshooting",
    "title": "Troubleshooting Grapheteria",
    "content": " ",
    "url": "/Core/UI/Troubleshooting.html#troubleshooting-grapheteria",
    
    "relUrl": "/Core/UI/Troubleshooting.html#troubleshooting-grapheteria"
  },"93": {
    "doc": "Troubleshooting",
    "title": "Overview",
    "content": "Welcome to the troubleshooting section! As more people try Grapheteria, we’ll keep adding commonly faced issues (and keep trying to fix them, of course!). The golden rule of bug fixing: when things go from bad to worse, a good old server restart fixes most issues. A restart triggers a fresh scan of all workflows and nodes in your directory, ensuring you’re working with the latest data. Not ideal, but effective! . ",
    "url": "/Core/UI/Troubleshooting.html#overview",
    
    "relUrl": "/Core/UI/Troubleshooting.html#overview"
  },"94": {
    "doc": "Troubleshooting",
    "title": "Module Loading Errors",
    "content": "If your Python module containing node definitions has errors, it won’t load properly: . # This broken code will prevent nodes from being registered class MyBrokenNode(Node): def execute(self, prepared_result) # Missing colon here! return \"This won't work\" . Symptoms: . | Nodes don’t appear in the UI | Old versions of nodes show up instead of your updates | Deleted nodes stubbornly remain in the UI | . Solution: Check for syntax errors in your Python files. The compiler’s red squiggles are your friends here! Fix any syntax issues and re-save the file. ",
    "url": "/Core/UI/Troubleshooting.html#module-loading-errors",
    
    "relUrl": "/Core/UI/Troubleshooting.html#module-loading-errors"
  },"95": {
    "doc": "Troubleshooting",
    "title": "Deleted Node Classes",
    "content": "When you delete a node class from your Python code, it may haunt your UI: . # If you delete this from your code... class DataProcessorNode(Node): async def execute(self, prepared_result): return process_data(prepared_result) . The node will continue to appear in your workflow until you explicitly remove it from the UI or the JSON file. Why? This preserves your workflow structure in case you accidentally delete code. Automatic removal would cascade to connected edges and potentially break your workflow. Solution: Manually delete unwanted nodes from the UI canvas or edit your workflow JSON file. ",
    "url": "/Core/UI/Troubleshooting.html#deleted-node-classes",
    
    "relUrl": "/Core/UI/Troubleshooting.html#deleted-node-classes"
  },"96": {
    "doc": "Troubleshooting",
    "title": "File Renaming Adventures",
    "content": "Renaming files can lead to unexpected behavior: . What happens: The server tracks each file and its nodes in a dictionary structure. When you rename a file: . | The old filename entry remains in the server’s memory | A new entry is created for the new filename | This causes node duplication in the tracking system | . This duplication can lead to unexpected behavior, like nodes appearing twice or importing failures. Solution: After renaming files, restart the server to clear its internal file registry and rebuild it from scratch. ",
    "url": "/Core/UI/Troubleshooting.html#file-renaming-adventures",
    
    "relUrl": "/Core/UI/Troubleshooting.html#file-renaming-adventures"
  },"97": {
    "doc": "Troubleshooting",
    "title": "Duplicate Node Names",
    "content": "Having multiple node classes with the same name across different files creates confusion: . # In file1.py class ProcessorNode(Node): # Does one thing # In file2.py class ProcessorNode(Node): # Same name! # Does something entirely different . What happens: Only one version will appear in the UI - typically the last one scanned. This behavior isn’t deterministic and can lead to unexpected workflow behavior. Solution: Use unique, descriptive names for your node classes: . # Better naming class TextProcessorNode(Node): ... class ImageProcessorNode(Node): ... ",
    "url": "/Core/UI/Troubleshooting.html#duplicate-node-names",
    
    "relUrl": "/Core/UI/Troubleshooting.html#duplicate-node-names"
  },"98": {
    "doc": "Troubleshooting",
    "title": "The Universal Fix: Server Restart",
    "content": "When all else fails, restart your server: . # Stop current server (Ctrl+C) grapheteria . This ensures all your code is freshly scanned and registered properly. ",
    "url": "/Core/UI/Troubleshooting.html#the-universal-fix-server-restart",
    
    "relUrl": "/Core/UI/Troubleshooting.html#the-universal-fix-server-restart"
  },"99": {
    "doc": "Troubleshooting",
    "title": "Getting Help",
    "content": "Encountered an issue not covered here? Please report it on our GitHub Issues page. Your feedback helps make Grapheteria better for everyone! . Happy graph-building! 🚀 . ",
    "url": "/Core/UI/Troubleshooting.html#getting-help",
    
    "relUrl": "/Core/UI/Troubleshooting.html#getting-help"
  },"100": {
    "doc": "Troubleshooting",
    "title": "Troubleshooting",
    "content": " ",
    "url": "/Core/UI/Troubleshooting.html",
    
    "relUrl": "/Core/UI/Troubleshooting.html"
  },"101": {
    "doc": "Workflow Engine",
    "title": "Workflow Orchestration",
    "content": " ",
    "url": "/Core/Workflow.html#workflow-orchestration",
    
    "relUrl": "/Core/Workflow.html#workflow-orchestration"
  },"102": {
    "doc": "Workflow Engine",
    "title": "Bringing It All Together: The Workflow Engine",
    "content": "Now that we’ve defined our nodes, edges, and shared variables, it’s time to fire up the engine! The WorkflowEngine class is where all the magic happens - it’s the conductor that orchestrates your workflow from start to finish. from grapheteria import WorkflowEngine from nodes import Node1, Node2, Node3 # Create nodes start_node = Node1(id=\"start\") middle_node = Node2(id=\"process\") end_node = Node3(id=\"finish\") start_node &gt; middle_node &gt; end_node #Connect them shared_initial = {\"counter\": 0} #Initialize the shared variable # Initialize the engine with our nodes engine = WorkflowEngine( nodes=[start_node, middle_node, end_node], start=start_node, initial_shared_state=shared_initial ) . ",
    "url": "/Core/Workflow.html#bringing-it-all-together-the-workflow-engine",
    
    "relUrl": "/Core/Workflow.html#bringing-it-all-together-the-workflow-engine"
  },"103": {
    "doc": "Workflow Engine",
    "title": "JSON or Code? Choose Your Adventure",
    "content": "The workflow engine is flexible - you can define workflows either through code (as shown above) or via JSON files. Code-Based Workflows . With code-based workflows, you directly pass your node objects as a list: . # Create your node instances node1 = TextProcessorNode(id=\"process_text\") node2 = DatabaseNode(id=\"save_results\") node3 = NotificationNode(id=\"notify_user\") # The workflow_id is optional - a random one will be generated if not provided engine = WorkflowEngine( nodes=[node1, node2, node3], workflow_id=\"my_awesome_workflow\" ) . JSON-Based Workflows . You might have a JSON file with the following format - . { \"nodes\": [\"...\"], \"edges\": [\"...\"], \"initial_state\": {\"...\"}, \"start\": \"...\" } . For JSON-based workflows, you need to import all node classes first, then provide the path or ID . # IMPORTANT: Import all node classes used in your JSON workflow from my_nodes import TextProcessorNode, DatabaseNode, NotificationNode from more_nodes import ValidationNode, TransformationNode # The imports register the nodes with Grapheteria's registry # Now you can load the workflow without initializing any nodes yourself # Using workflow_path engine = WorkflowEngine(workflow_path=\"workflows/data_pipeline.json\") # Or using workflow_id (will look for workflows/data_pipeline.json) engine = WorkflowEngine(workflow_id=\"workflows.data_pipeline\") . This works because Grapheteria automatically registers node classes when they’re imported. The engine then instantiates the right classes based on the JSON definition. The conversion between path and id is simple: dots in IDs become slashes in paths, making workflows.data_pipeline equal to workflows/data_pipeline.json. ",
    "url": "/Core/Workflow.html#json-or-code-choose-your-adventure",
    
    "relUrl": "/Core/Workflow.html#json-or-code-choose-your-adventure"
  },"104": {
    "doc": "Workflow Engine",
    "title": "Defining Your Workflow’s Entry Point",
    "content": "Every workflow needs a starting point. If you don’t explicitly set one (not recommended), the engine will default to the first node in your list . # Explicitly setting the start node (recommended) engine = WorkflowEngine( nodes=[node1, node2, node3], start=node2 # We're starting with node2, not node1 ) # Without setting start - will use the first node in the list engine = WorkflowEngine(nodes=[node1, node2, node3]) # Will start with node1 . ",
    "url": "/Core/Workflow.html#defining-your-workflows-entry-point",
    
    "relUrl": "/Core/Workflow.html#defining-your-workflows-entry-point"
  },"105": {
    "doc": "Workflow Engine",
    "title": "Ready, Set, Start!: Baby Steps or Full Speed",
    "content": "The engine gives you two ways to run your workflow: . Take One Step at a Time . Perfect for debugging or when you need granular control: . # Execute just one node and stop continuing, _ = await engine.step() . Full Speed Ahead . When you’re ready to let it rip: . # Run the entire workflow until completion or until input is required continuing, _ = await engine.run() . For providing inputs when a node awaits request_input(), both the step() and run() functions take in a optional parameter called input_data. Check out Human-in-the-Loop for a more detailed explanation. ",
    "url": "/Core/Workflow.html#ready-set-start-baby-steps-or-full-speed",
    
    "relUrl": "/Core/Workflow.html#ready-set-start-baby-steps-or-full-speed"
  },"106": {
    "doc": "Workflow Engine",
    "title": "Monitoring Workflow State",
    "content": "The heart of the engine is the execution_state, which gives you real-time information about what’s happening in your workflow: . # Check the current status of your workflow workflow_status = engine.execution_state.workflow_status print(f\"Current status: {workflow_status}\") # RUNNING, COMPLETED, WAITING_FOR_INPUT, etc. # See what node is up next next_node = engine.execution_state.next_node_id print(f\"Next node to execute: {next_node}\") # Inspect the shared variables shared_data = engine.execution_state.shared print(f\"Current counter value: {shared_data['counter']}\") # Check if the workflow is waiting for user input if engine.execution_state.workflow_status == WorkflowStatus.WAITING_FOR_INPUT: input_request = engine.execution_state.awaiting_input node_id = input_request['node_id'] prompt = input_request['prompt'] print(f\"Workflow is asking: {prompt}\") # Provide the requested input and continue user_response = input(\"&gt; \") continuing, _ = await engine.step({node_id: user_response}) . The engine also maintains a complete history in tracking_data (we cover this in more detail in the next section), which captures the entire journey of your workflow for analysis and resumability. Enjoy running your workflows! With Grapheteria’s powerful engine, you can orchestrate complex state machines with confidence and control. ",
    "url": "/Core/Workflow.html#monitoring-workflow-state",
    
    "relUrl": "/Core/Workflow.html#monitoring-workflow-state"
  },"107": {
    "doc": "Workflow Engine",
    "title": "Workflow Engine",
    "content": " ",
    "url": "/Core/Workflow.html",
    
    "relUrl": "/Core/Workflow.html"
  },"108": {
    "doc": "Concepts",
    "title": "Welcome to the Concepts Documentation",
    "content": "This section goes over some essential concepts needed to understand Grapheteria and agentic workflows in general. We’ll constantly keep adding to this section to help you become a workflow wizard! . ",
    "url": "/Concepts/#welcome-to-the-concepts-documentation",
    
    "relUrl": "/Concepts/#welcome-to-the-concepts-documentation"
  },"109": {
    "doc": "Concepts",
    "title": "What’s Inside",
    "content": "State Machines . The building blocks of Grapheteria - discover how these simple but powerful structures let you create complex, reliable workflows that mirror real-world processes. ",
    "url": "/Concepts/#whats-inside",
    
    "relUrl": "/Concepts/#whats-inside"
  },"110": {
    "doc": "Concepts",
    "title": "Why These Concepts Matter",
    "content": "Understanding these fundamentals will transform how you think about automation. Instead of writing complex code, you’ll be designing intuitive workflows that handle complexity for you. Thanks for visiting, and happy workflow building! . ",
    "url": "/Concepts/#why-these-concepts-matter",
    
    "relUrl": "/Concepts/#why-these-concepts-matter"
  },"111": {
    "doc": "Concepts",
    "title": "Concepts",
    "content": " ",
    "url": "/Concepts/",
    
    "relUrl": "/Concepts/"
  },"112": {
    "doc": "Advanced",
    "title": "Advanced Features",
    "content": "Welcome to the advanced documentation. This goes over all the advanced concepts to further extend grapheteria’s core features. These set of docs assumes you already understand the basic features and want more! If that’s you, please read along. And once again, thanks for using our library! . ",
    "url": "/Advanced/#advanced-features",
    
    "relUrl": "/Advanced/#advanced-features"
  },"113": {
    "doc": "Advanced",
    "title": "Your Advanced Journey Begins",
    "content": "Ready to supercharge your state machine? You’ve mastered the basics, created some workflows, and now you’re thinking, “What else can this thing do?” Turns out, quite a lot! Think of this section as the secret menu at your favorite restaurant - all the good stuff that the regulars know about. ",
    "url": "/Advanced/#your-advanced-journey-begins",
    
    "relUrl": "/Advanced/#your-advanced-journey-begins"
  },"114": {
    "doc": "Advanced",
    "title": "What’s Inside",
    "content": "Advanced Nodes . Take your state machine to the next level with specialized nodes that handle complex operations. Learn how to create custom nodes that fit your unique workflow needs. Extending Logging . Debug like a pro. Find out how to customize logging to track exactly what you need, store where you need, without drowning in data. Human-in-the-Loop . Sometimes humans need to join the party! Learn how to seamlessly integrate human decision points into your automated workflows. Deployment Strategies . Take your state machine from your laptop to the world. Explore different deployment options and best practices for production environments. Event-Driven Architecture . Make your state machine responsive to the world around it. Discover how to build systems that react to events in real-time without breaking a sweat. ",
    "url": "/Advanced/#whats-inside",
    
    "relUrl": "/Advanced/#whats-inside"
  },"115": {
    "doc": "Advanced",
    "title": "Growing Together",
    "content": "We’re constantly working to add new features and improve grapheteria’s capabilities. Our goal is to make your experience with state machines better, easier, and more powerful with each update. Have an idea for a feature that would make your workflow smoother? Something you wish grapheteria could do? We’d love to hear about it! Please reach out in the GitHub discussions page to share your suggestions. Your feedback directly shapes the future of this library. ",
    "url": "/Advanced/#growing-together",
    
    "relUrl": "/Advanced/#growing-together"
  },"116": {
    "doc": "Advanced",
    "title": "Advanced",
    "content": " ",
    "url": "/Advanced/",
    
    "relUrl": "/Advanced/"
  },"117": {
    "doc": "UI",
    "title": "UI",
    "content": "Welcome to the visual side of Grapheteria! The UI connects to your codebase through websockets, making your workflow design interactive and intuitive. To get started, run this command in your terminal: . grapheteria . This simple command launches both the server and UI. Now you can visually manage all your nodes, create connections, configure them, and debug your workflows with ease! . ",
    "url": "/Core/UI/",
    
    "relUrl": "/Core/UI/"
  },"118": {
    "doc": "Core",
    "title": "Core Concepts: Your Journey Through Grapheteria",
    "content": "Welcome to the heart of Grapheteria! This section covers all the essential concepts you need to build amazing workflows. Grab a coffee, get comfortable, and let’s dive in - we promise it’ll be a smooth ride! . ",
    "url": "/Core/#core-concepts-your-journey-through-grapheteria",
    
    "relUrl": "/Core/#core-concepts-your-journey-through-grapheteria"
  },"119": {
    "doc": "Core",
    "title": "Your Roadmap",
    "content": "This documentation follows a natural progression: . | Nodes - The building blocks of your workflow | Edges - Connecting your nodes together | Shared - Data that flows through your workflow | Workflows - Assembling the complete picture | Logging - Keeping track of what’s happening | UI - Bringing it all to life visually | . Each concept builds on the previous one, so reading in order will give you the best understanding. Let’s start the adventure! . ",
    "url": "/Core/#your-roadmap",
    
    "relUrl": "/Core/#your-roadmap"
  },"120": {
    "doc": "Core",
    "title": "Core",
    "content": " ",
    "url": "/Core/",
    
    "relUrl": "/Core/"
  },"121": {
    "doc": "Home",
    "title": "Welcome to Grapheteria!",
    "content": "Thanks for checking out this project! We’re excited to show you what we’ve built. ",
    "url": "/#welcome-to-grapheteria",
    
    "relUrl": "/#welcome-to-grapheteria"
  },"122": {
    "doc": "Home",
    "title": "What is Grapheteria?",
    "content": "Grapheteria is a no-bs library for creating agentic workflows. It helps you design, visualize, and execute complex processes with minimal fuss. “Oh no, not ANOTHER workflow library!” - I hear you. But being late to the game has its advantages. We’ve learned from others’ mistakes and borrowed their best ideas, while introducing some genuinely new features. ",
    "url": "/#what-is-grapheteria",
    
    "relUrl": "/#what-is-grapheteria"
  },"123": {
    "doc": "Home",
    "title": "Why Grapheteria?",
    "content": "While there are standards for agent creation and tool-calling, workflow creation remains fragmented. Grapheteria aims to change that with a clean, generic framework that provides essential features while remaining infinitely extensible. Problems with Existing Tools . Code-based workflow builders often drown you in abstractions: . | “Wait, what does this wrapper do again?” | “How many layers of inheritance am I dealing with?” | “I just want to see what’s happening!” | . UI-only tools hit a ceiling: . | Limited customization for complex scenarios | Multi-agent setups become impossible | You eventually end up back in code anyway | . ",
    "url": "/#why-grapheteria",
    
    "relUrl": "/#why-grapheteria"
  },"124": {
    "doc": "Home",
    "title": "The Vision: Best of Both Worlds",
    "content": "Grapheteria seamlessly blends code and UI. Freely move between visual design and code customization without compromise. Get the full power of code with the clarity of visual debugging. Grapheteria uses state machines to define workflows - a powerful pattern that makes complex processes manageable and predictable. Learn more about state machines in Grapheteria. ",
    "url": "/#the-vision-best-of-both-worlds",
    
    "relUrl": "/#the-vision-best-of-both-worlds"
  },"125": {
    "doc": "Home",
    "title": "Standout Features",
    "content": "Clean, Simple Code . Write workflows without learning a complex API first: . start_node = InputNode(id=\"get_name\") process_node = ProcessNode(id=\"greet\") output_node = OutputNode(id=\"display\") start_node &gt; process_node &gt; output_node . Visual Workflow Design . Edit your workflows visually or programmatically - they stay in sync! . Time-Travel Debugging . Made a mistake? No problem: . | Step backwards in your workflow | Fix the issue | Step forwards | Continue from exactly where you left off | . Built-in Essentials . | Comprehensive logging | Automatic state persistence | Easy resumption of workflows | . Production-Ready Path . From prototype to production with minimal changes: . | Scale from local to distributed execution | Monitor and track workflow performance | Handle errors gracefully | . Vibe-Coding Compatible . Describe what you want, then refine: . | Generate workflow skeletons with AI | Modify rather than starting from scratch | Rapidly prototype complex workflows | . ",
    "url": "/#standout-features",
    
    "relUrl": "/#standout-features"
  },"126": {
    "doc": "Home",
    "title": "Ready to Try It?",
    "content": "Installation . # Create a virtual environment (recommended) python -m venv venv source venv/bin/activate # On Windows: venv\\Scripts\\activate # Install Grapheteria pip install grapheteria . Note: Grapheteria requires Python 3.6 or higher. Launch the UI . Once installed, launch the UI with: . grapheteria . This will start the Grapheteria interface and automatically sync with your codebase. Learn Core Concepts → ",
    "url": "/#ready-to-try-it",
    
    "relUrl": "/#ready-to-try-it"
  },"127": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"128": {
    "doc": "State Machines",
    "title": "The Magic of State Machines: Building Intelligent Workflows",
    "content": " ",
    "url": "/Concepts/state_machines.html#the-magic-of-state-machines-building-intelligent-workflows",
    
    "relUrl": "/Concepts/state_machines.html#the-magic-of-state-machines-building-intelligent-workflows"
  },"129": {
    "doc": "State Machines",
    "title": "Overview",
    "content": "Grapheteria uses something called “finite state machines” to create workflows that can act on their own (that’s what “agentic” means). Think of it as building a flow chart where each step (node) connects to other steps through paths (edges), creating a system that can make decisions and do tasks automatically. ",
    "url": "/Concepts/state_machines.html#overview",
    
    "relUrl": "/Concepts/state_machines.html#overview"
  },"130": {
    "doc": "State Machines",
    "title": "What’s a State Machine?",
    "content": "Imagine a traffic light. It can only be in one state at a time: red, yellow, or green. After a set time, it changes from one state to another in a specific order. That’s a simple state machine! . Another example is your washing machine: . | It starts in the “off” state | You put in clothes and press start → it moves to the “washing” state | After washing, it automatically moves to the “rinsing” state | Then it goes to the “spinning” state | Finally, it returns to the “off” state | . State machines are all around us - they’re just systems that can be in exactly one state at any time, with clear rules about how they move between states. ",
    "url": "/Concepts/state_machines.html#whats-a-state-machine",
    
    "relUrl": "/Concepts/state_machines.html#whats-a-state-machine"
  },"131": {
    "doc": "State Machines",
    "title": "Grapheteria’s Implementation",
    "content": "In Grapheteria, we build these state machines as workflows: . # Create two simple steps (nodes) check_weather = WeatherNode(id=\"check_weather\") decide_activity = DecisionNode(id=\"decide_activity\") # Connect them (the arrow means \"go from this step to that step\") check_weather &gt; decide_activity # Or make the connection depend on certain conditions check_weather - \"shared['weather'] == 'sunny'\" &gt; go_to_beach check_weather - \"shared['weather'] == 'rainy'\" &gt; stay_indoors . Nodes as States . A node is just a single step in your workflow. Each node has one job: . | Get the weather forecast | Ask a human for input | Decide what to do next | Generate a response using AI | . Think of each node like a station in an assembly line - it does its specific task, then passes things along to the next station. Edges as Transitions . Edges are the connections between nodes - they’re like the conveyor belts in our assembly line example. What makes them special is they can have conditions: . payment_check - \"payment_successful\" &gt; send_confirmation payment_check - \"payment_failed\" &gt; retry_payment . This means your workflow can take different paths depending on what happens at each step - just like how you might take a different route home if there’s traffic on your usual road. ",
    "url": "/Concepts/state_machines.html#grapheterias-implementation",
    
    "relUrl": "/Concepts/state_machines.html#grapheterias-implementation"
  },"132": {
    "doc": "State Machines",
    "title": "Why a State Machine?",
    "content": "Why do we build workflows this way? Because it makes complicated things simple: . | Easy to Understand: You can draw your workflow on a piece of paper with boxes and arrows. Anyone can understand it without knowing code. | Mirrors Real Life: When you make breakfast, you’re following a workflow: get eggs → crack eggs → cook eggs → serve eggs. Life is full of workflows! . | Reliable: State machines always behave the same way given the same inputs, making them dependable. | Can Pause and Resume: Imagine if your GPS could pause during your road trip and continue exactly where you left off next week. Grapheteria workflows can do that: . | . # Pick up a workflow where you left off engine = WorkflowEngine( workflow_id=\"signup_process\", run_id=\"20230615_103045\" ) # Continue running it is_active, data = await engine.run() . State machines give you a simple way to build powerful systems that can handle complex tasks. They’re like Lego blocks for creating processes that work exactly how you want them to. ",
    "url": "/Concepts/state_machines.html#why-a-state-machine",
    
    "relUrl": "/Concepts/state_machines.html#why-a-state-machine"
  },"133": {
    "doc": "State Machines",
    "title": "State Machines",
    "content": " ",
    "url": "/Concepts/state_machines.html",
    
    "relUrl": "/Concepts/state_machines.html"
  }
}
